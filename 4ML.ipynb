{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITa8_YdqNeN6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "pd.set_option('display.max_colwidth', 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0ltLQGlQMEx",
        "outputId": "e0b51004-ef07-47b3-8055-5f03807b23f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from textblob import Word\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import csv\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NihL6ijyNiLa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/Covid-19 Twitter Dataset (Aug-Sep 2020).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_a_reajQKlA",
        "outputId": "a68b11d8-8d3b-43df-e4b9-433ae050397c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DSPql6TxQDK"
      },
      "outputs": [],
      "source": [
        "tweets_column = df['clean_tweet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XqQ8FGaxSH5"
      },
      "outputs": [],
      "source": [
        "compound = df['compound']\n",
        "sentiment = df['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RuQVyBXxVfq",
        "outputId": "33210b0f-b4a3-431b-ec05-0188bf6bf55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                 year old ex vice presid moodi awori land inter counti covid19 committe role\n",
            "1                      break depart health report peopl caught covid19 push nation case count\n",
            "2                                    help fan request help arrang bed posit father view tweet\n",
            "3                                                              lend club loan origin hey bank\n",
            "4                     curiou case unit nation ngo appreci kingdom mamata banerje covid19 work\n",
            "                                                 ...                                         \n",
            "120504                      exclus astrazeneca covid19 vaccin trial may resum soon week sourc\n",
            "120505                         worker countri sign petit reinstat worker mass fire request co\n",
            "120506    cultur china brillianc huax celebr th anniversari canada china diplomat relat fight\n",
            "120507                               trump call cnn bastard cover covid19 hear keilar respons\n",
            "120508                           trump call cnn bastard cover covid19 hear keilar respons via\n",
            "Name: clean_tweet, Length: 120509, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(tweets_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECarTvXIxWOe",
        "outputId": "5ef0c80e-fcd3-4342-ef32-7c1249a27507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         0.0000\n",
            "1         0.0000\n",
            "2         0.7717\n",
            "3         0.0000\n",
            "4         0.0000\n",
            "           ...  \n",
            "120504    0.0000\n",
            "120505   -0.3400\n",
            "120506   -0.3818\n",
            "120507   -0.5423\n",
            "120508   -0.5423\n",
            "Name: compound, Length: 120509, dtype: float64\n",
            "0         neu\n",
            "1         neu\n",
            "2         pos\n",
            "3         neu\n",
            "4         neu\n",
            "         ... \n",
            "120504    neu\n",
            "120505    neg\n",
            "120506    neg\n",
            "120507    neg\n",
            "120508    neg\n",
            "Name: sentiment, Length: 120509, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(compound)\n",
        "print(sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aECz-6MMsnNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaPmNIxgxadV"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "def processRow(row):\n",
        "    row = row.lower()\n",
        "    row = re.sub(r'(\\\\u[0-9A-Fa-f]+)', \"\", row)\n",
        "    row = re.sub(r'[^\\x00-\\x7f]', \"\", row)\n",
        "    row = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', row)\n",
        "    row = re.sub('@[^\\s]+', 'AT_USER', row)\n",
        "    row = re.sub(r'[\\s]+', ' ', row)\n",
        "    row = re.sub(r'[\\n]+', ' ', row)\n",
        "    row = re.sub(r'[^\\w]', ' ', row)\n",
        "    row = re.sub(r'#([^\\s]+)', r'\\1', row)\n",
        "    row = row.replace(':)', \"\")\n",
        "    row = row.replace(':(', \"\")\n",
        "    row = \"\".join([i for i in row if not i.isdigit()])\n",
        "    row = re.sub(r\"(\\!)\\1+\", ' ', row)\n",
        "    row = re.sub(r\"(\\?)\\1+\", ' ', row)\n",
        "    row = re.sub(r\"(\\.)\\1+\", ' ', row)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenizer = TweetTokenizer()\n",
        "\n",
        "    words = tokenizer.tokenize(row)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    filtered_sentence = ' '.join(filtered_words)\n",
        "    return filtered_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFXhuHhdxnaO"
      },
      "outputs": [],
      "source": [
        "data = tweets_column.apply(lambda x: processRow(str(x)) if isinstance(x, str) else str(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqd2cbTzxn7G"
      },
      "outputs": [],
      "source": [
        "datas=data.apply(processRow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUqNME5IxqJZ",
        "outputId": "aabff5a6-169a-46da-9a86-7e8a081297f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tweets=datas\n",
        "\n",
        "lemmatized_tweets = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    words = word_tokenize(tweet)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    lemmatized_tweet = ' '.join(lemmatized_words)\n",
        "    lemmatized_tweets.append(lemmatized_tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4UEWKhkxsEC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oB-Y5nlxukE"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentences = lemmatized_tweets\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_sentences = []\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    stemmed_sentence = ' '.join(stemmed_words)\n",
        "    stemmed_sentences.append(stemmed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7m3ioFSxwU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286e0bec-c532-4619-ca9e-4d9d9f529fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = stemmed_sentences\n",
        "\n",
        "all_words = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    all_words.extend(words)\n",
        "\n",
        "print(\"Extracted words:\", all_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2zjKzWn20Sx",
        "outputId": "a0d19d74-1a3d-462b-98c3-16def6be133d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35021\n",
            "31151\n"
          ]
        }
      ],
      "source": [
        "postive={}\n",
        "negative={}\n",
        "neutral={}\n",
        "for i,j in zip(sentiment,lemmatized_tweets):\n",
        "  if i =='pos':\n",
        "    postive[j]='positive'\n",
        "  elif i=='neg':\n",
        "    negative[j]='negative'\n",
        "  else:\n",
        "    neutral[j]='neutral'\n",
        "print(len(postive))\n",
        "print(len(negative))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data=[]\n",
        "label=[]\n",
        "for i,j in zip(sentiment,lemmatized_tweets):\n",
        "  if i =='pos':\n",
        "    tweet_data.append(j);\n",
        "    label.append('positive')\n",
        "  elif i=='neg':\n",
        "    tweet_data.append(j);\n",
        "    label.append('negative')\n",
        "print(len(tweet_data))\n",
        "print(len(label))\n",
        "print(tweet_data[1])\n",
        "print(label[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFunxz506q85",
        "outputId": "0e26c539-34b8-4132-e741-5318275cf39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69515\n",
            "69515\n",
            "jtbc drama privat life star snsd seohyun go kyungpyo stop film due covid concern wi\n",
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkryNJ9l7ZmV",
        "outputId": "9e069410-138c-4fc1-fc49-48b23002456e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "4ahQuTZTg64b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvCZGedFt_hf",
        "outputId": "2466311d-62f8-4866-a365-987792e1f092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Accuracy: 0.88\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "combined_train_texts, combined_test_texts, combined_train_labels, combined_test_labels = train_test_split(\n",
        "    tweet_data,label, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "combined_train_features = vectorizer.fit_transform(combined_train_texts)\n",
        "combined_test_features = vectorizer.transform(combined_test_texts)\n",
        "nb_combined_model = MultinomialNB()\n",
        "nb_combined_model.fit(combined_train_features, combined_train_labels)\n",
        "\n",
        "combined_test_predictions = nb_combined_model.predict(combined_test_features)\n",
        "\n",
        "\n",
        "combined_accuracy = accuracy_score(combined_test_labels, combined_test_predictions)\n",
        "print(f\"Combined Accuracy: {combined_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear SVC"
      ],
      "metadata": {
        "id": "RoB6CeTVgw-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu4kA0lWzGAb",
        "outputId": "4169bede-6c2c-4d8f-9d83-0a818e587128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Accuracy using LinearSVC: 0.97\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "positive_data =postive\n",
        "negative_data = negative\n",
        "\n",
        "combined_data = {**positive_data, **negative_data}\n",
        "\n",
        "\n",
        "combined_texts = list(combined_data.keys())\n",
        "combined_labels = list(combined_data.values())\n",
        "\n",
        "\n",
        "combined_train_texts, combined_test_texts, combined_train_labels, combined_test_labels = train_test_split(\n",
        "    tweet_data, label, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "combined_train_features = vectorizer.fit_transform(combined_train_texts)\n",
        "combined_test_features = vectorizer.transform(combined_test_texts)\n",
        "\n",
        "svc_model = LinearSVC()\n",
        "svc_model.fit(combined_train_features, combined_train_labels)\n",
        "\n",
        "combined_test_predictions = svc_model.predict(combined_test_features)\n",
        "\n",
        "\n",
        "combined_accuracy = accuracy_score(combined_test_labels, combined_test_predictions)\n",
        "print(f\"Combined Accuracy using LinearSVC: {combined_accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "PBc_FkKygXfb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iNYpmWYU7Gg",
        "outputId": "e1f80049-3810-455e-eb5b-d57900a1b977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using RandomForestClassifier: 0.94\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "    tweet_data, label, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "train_features_tfidf = vectorizer.fit_transform(train_features)\n",
        "test_features_tfidf = vectorizer.transform(test_features)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "rf_model.fit(train_features_tfidf, train_labels)\n",
        "\n",
        "\n",
        "test_predictions = rf_model.predict(test_features_tfidf)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Accuracy using RandomForestClassifier: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "JzkBqaj8gmkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTVI1Xxgeo6L",
        "outputId": "b4ad296c-9d86-45ba-ec3e-c71df91f6fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Logistic Regression: 0.96\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "combined_features = vectorizer.fit_transform(tweet_data)\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "    combined_features, label, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "logistic_combined_model = LogisticRegression()\n",
        "logistic_combined_model.fit(train_features, train_labels)\n",
        "\n",
        "\n",
        "test_predictions = logistic_combined_model.predict(test_features)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(f\"Accuracy using Logistic Regression: {accuracy:.2f}\")\n"
      ]
    }
  ]
}